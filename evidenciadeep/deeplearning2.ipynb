{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocm/Desktop/03_ESCUELA/02_Universidad/07/Base de datos/DeepLearning/evidenciadeep/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/dd/h54l8ct14j32kfynw6hg9f100000gn/T/ipykernel_42963/121112958.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  liar_dataset_cleaned['speaker\\'s job title'].fillna('Unknown', inplace=True)\n",
      "/var/folders/dd/h54l8ct14j32kfynw6hg9f100000gn/T/ipykernel_42963/121112958.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  liar_dataset_cleaned['state info'].fillna('Unknown', inplace=True)\n",
      "/var/folders/dd/h54l8ct14j32kfynw6hg9f100000gn/T/ipykernel_42963/121112958.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  liar_dataset_cleaned['venue'].fillna('Unknown', inplace=True)\n",
      "2024-11-07 17:10:36.357011: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-11-07 17:10:36.357038: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2024-11-07 17:10:36.357046: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2024-11-07 17:10:36.357078: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-07 17:10:36.357088: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 17:14:17.184447: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.5629598498344421\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 17:17:51.545302: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.2551004886627197\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 17:21:22.333382: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.0071550593711435795\n",
      "Accuracy en el conjunto de prueba: 0.6161063313484192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 17:21:29.954807: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Paso 1: Cargar y limpiar el dataset\n",
    "data = pd.read_csv('dataset/Liar_Dataset.csv')\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "columns_to_drop = ['[ID].json', 'barely true counts', 'false counts', \n",
    "                   'half true counts', 'mostly true counts', 'pants on fire counts']\n",
    "liar_dataset_cleaned = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Rellenar valores nulos y limpiar el texto\n",
    "liar_dataset_cleaned['speaker\\'s job title'].fillna('Unknown', inplace=True)\n",
    "liar_dataset_cleaned['state info'].fillna('Unknown', inplace=True)\n",
    "liar_dataset_cleaned['venue'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Mapeo de etiquetas a 'TRUE' o 'FALSE'\n",
    "label_mapping = {\n",
    "    'TRUE': 'TRUE',\n",
    "    'mostly-true': 'TRUE',\n",
    "    'FALSE': 'FALSE',\n",
    "    'barely-true': 'FALSE',\n",
    "    'half-true': 'FALSE',\n",
    "    'pants-fire': 'FALSE'\n",
    "}\n",
    "liar_dataset_cleaned['label'] = liar_dataset_cleaned['label'].map(label_mapping)\n",
    "\n",
    "# Limpieza de texto\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remueve caracteres especiales\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remueve espacios adicionales\n",
    "    return text\n",
    "\n",
    "liar_dataset_cleaned['statement'] = liar_dataset_cleaned['statement'].apply(clean_text)\n",
    "\n",
    "# Paso 2: Codificación de etiquetas y división en entrenamiento/prueba\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "liar_dataset_cleaned['label_encoded'] = label_encoder.fit_transform(liar_dataset_cleaned['label'])\n",
    "\n",
    "X = liar_dataset_cleaned['statement']\n",
    "y = liar_dataset_cleaned['label_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 3: Cargar DistilBERT y Tokenizar\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "\n",
    "def tokenize_texts(texts, tokenizer, max_length=64):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train, tokenizer)\n",
    "test_encodings = tokenize_texts(X_test, tokenizer)\n",
    "\n",
    "# Paso 4: Configurar el optimizador y la pérdida\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Paso 5: Entrenar el modelo usando GradientTape\n",
    "batch_size = 16\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
    "    y_train\n",
    ")).batch(batch_size)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for batch in train_dataset:\n",
    "        inputs, labels = batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = distilbert_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']).logits\n",
    "            loss = loss_fn(labels, logits)\n",
    "        gradients = tape.gradient(loss, distilbert_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, distilbert_model.trainable_variables))\n",
    "    print(f\"Loss after epoch {epoch + 1}: {loss.numpy()}\")\n",
    "\n",
    "# Paso 6: Evaluar el modelo en el conjunto de prueba\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},\n",
    "    y_test\n",
    ")).batch(batch_size)\n",
    "\n",
    "accuracy_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "for batch in test_dataset:\n",
    "    inputs, labels = batch\n",
    "    logits = distilbert_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']).logits\n",
    "    predictions = tf.round(tf.nn.sigmoid(logits))  # Convertir logits a etiquetas binarias\n",
    "    accuracy_metric.update_state(labels, predictions)\n",
    "\n",
    "print(f\"Accuracy en el conjunto de prueba: {accuracy_metric.result().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 17:32:18.683821: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.685687780380249\n",
      "Epoch 2/5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "optimizer = AdamW(learning_rate=2e-5)\n",
    "optimizer.build(distilbert_model.trainable_variables)\n",
    "\n",
    "# Continuar con el entrenamiento\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for batch in train_dataset:\n",
    "        inputs, labels = batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = distilbert_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']).logits\n",
    "            loss = loss_fn(labels, logits)\n",
    "        gradients = tape.gradient(loss, distilbert_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, distilbert_model.trainable_variables))\n",
    "    print(f\"Loss after epoch {epoch + 1}: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evidenciadeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
